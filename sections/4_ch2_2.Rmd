---
output: bookdown::pdf_document2
---

```{r child = 'code/4_ch2_2_code.Rmd', cache = TRUE}
```

## Experiment 2: Investigating the specificity of generalization from exposure to Spanish-accented stops

### Methods

This experiment was the same as Experiment 1, with the exception of the Similarity factor.

#### Design

The Control level was removed, leaving just two levels of Similarity: Direct and Indirect.
Recall that the pseudowords in the Direct and Indirect conditions had the same onsets as the experimental real words (critical and competitor, respectively).
It is possible that this design disrupted adaptation to Spanish-accented VOTs by associating them with pseudowords.
To improve learning, we replaced them with pseudowords with control onsets.

```{r exp2-fig, fig.cap = "Experiment 2 design.", fig.pos="H", dpi = 300}
knitr::include_graphics("figures/diss_2.png")
```

#### Participants {#methods-pars-1b}

We recruited `r par_all[2]` participants through Prolific.
After removing ineligible participants, `r par_diff$elig_2[2]` remained.
After removing participants with poor data quality, `r par_qual[2]` remained for analysis (`r par_text$report[2]`)
All aspects of recruitment were the same as in Experiment 1.
The same group of participants that completed the experiment without the exposure phase in Experiment 1 was also used for comparison.

#### Materials and procedure

The talkers, stimuli, tasks, and procedure were the same as those in Experiment 1.
Removing the Control level of Similarity from the exposure phase left four between-subjects conditions: Direct-Variant, Direct-Invariant, Indirect-Variant, and Indirect-Invariant.
The 288 filler items remained the same.
The 72 experimental real words also remained the same.
The 72 experimental pseudowords had control onsets regardless of Similarity.
Talker assignment and counterbalancing was the same.
This resulted in 16 experimental lists for the exposure phase, one for each combination of exposure condition (4) and talker assignment (4).
All aspects of the test phase remained the same.

#### Analysis and predictions

Data processing, model fitting, and analysis all remained the same relative to Experiment 1; the only change was implementing sum contrasts for the two levels of Similarity.
Prior to analyzing exposure task performance for real words with experimental onsets, we removed responses with RTs less than 50 ms (*N* = `r dat_rem_exp$rem_1[2]`; `r dat_rem_exp$pct_1[2]`).
We then detected and removed outliers (*N* = `r dat_rem_exp$rem_2[2]`; `r dat_rem_exp$pct_2[2]`).
Prior to analyzing test task performance for critical prime-target pairs, we removed responses with RTs less than 50 ms (*N* = `r dat_rem_test$rem_1[2]`; `r dat_rem_test$pct_1[2]`).
We then detected and removed outliers (*N* = `r dat_rem_test$rem_2[2]`; `r dat_rem_test$pct_2[2]`).
For the test task analyses, effect sizes for pairwise comparisons were calculated with the *emmeans* package [@lenth2022].

### Results

#### Exposure

There were no effects of Variability or Similarity on accuracy.
There was a main effect of Similarity on RTs (`r exp_rt_1b_comp_form`), with faster RTs in Indirect conditions (`r exp_rt_1b_indirect`) than in Direct conditions (`r exp_rt_1b_direct`; `r exp_rt_1b_dir_indir`).

#### Test

For accuracy, we observed a significant interaction between Exposure and Target (`r test_acc_1b_comp_form`).
Pairwise comparisons within each level of Target did not reveal significant differences between the Test-only condition and any of the exposure conditions (*ps* > .05).
To investigate the source of the Exposure-Target interaction, we expanded the pairwise comparisons to include the contrasts between exposure conditions.
Here, we observed improvements in accuracy on Competitor targets, but not on Identity or Unrelated targets, after Direct-Invariant exposure.
The pairwise comparisons for Competitor targets can be found in Table \@ref(tab:exp2-test-tab).
Performance for each target type for each group is illustrated in Figure \@ref(fig:exp2-test-fig)
There were no effects of Exposure on RTs (*ps* > .05).

```{r exp2-test-tab, fig.pos="H"}
test_acc_1b_train_tab %>%
  dplyr::filter(match_type == "Competitor") %>%
  select(-match_type) %>%
  select(contrast, diff, z, p) %>%
  kable(col.names = c("Contrast", "Difference (accuracy)", "\\textit{z}", "\\textit{p}"),
        escape = FALSE,
        caption = "Experiment 2 accuracy differences, \\textit{z} ratios, and \\textit{p} values for contrasts between groups on Competitor targets.") %>%
  kable_styling(bootstrap_options = "bordered")
```

```{r exp2-test-fig, fig.cap = "Experiment 2 estimated marginal means and 95\\% CIs for test accuracy.", fig.pos="H", dpi = 300}
knitr::include_graphics("sections/code/outputs/train_plot_1b.png")
```

### Discussion {#discuss-1b}

In Experiment 2, two levels of Similarity---Direct and Indirect---were crossed with two levels of Variability---Variant and Invariant---in order to investigate how experience with Spanish-accented stops transfers to a novel talker.
The results of the exposure phase highlight how VOT influences the categorization of stop consonants.
Specifically, participants were faster to accurately categorize real words with voiced stops than with voiceless stops (Indirect exposure versus Direct exposure comparison).
This difference in RT suggests that associating VOTs with voiceless stops was relatively difficult for listeners compared to associating them with voiced stops.
Some of the difference in mean RTs likely reflects the temporal difference between voiced and voiceless stops articulation; however, we did not observe the same difference between Direct and Indirect exposure in Experiment 1, which featured the same real word stimuli.
This suggests that we can attribute the difference in RTs here to the process of learning to map VOT cues onto stop categories.
By contrast, we did not observe main effects of Similarity on accuracy during the exposure phase, which suggests that ultimate categorization did not differ between voiced and voiceless stops.
In other words, even though the *process* of cue-category mapping was more difficult for voiceless stops than for voiced stops, the *outcome* was the same.

This difference in exposure performance translated to a difference in test performance.
While the Direct-Invariant group did not significantly outperform the Test-only group, their performance on Competitor targets was numerically higher.
Moreover, the Direct-Invariant group clearly outperformed the Indirect-Invariant group, trended toward outperforming the Direct-Variant group, and numerically outperformed the Indirect-Variant group.
Together, these results suggest that Direct-Invariant exposure reduced lexical competition between voiced-voiceless minimal pairs.

In order to interpret this finding in terms of generalization, we will briefly describe the structure of the exposure conditions again.
Both Direct conditions exposed participants to Spanish-accented /p/, /t/, and /k/ in such disambiguating lexical contexts as *pencil*, *tablet*, and *kingdom*, respectively.
However, the Direct-Invariant condition in particular exposed participants to one-to-one mappings between each critical onset and each talker, such that all /p/ onsets were produced by Talker A, all /t/ onsets by Talker B, and all /k/ onsets by Talker C.
Thus, participants in the Direct-Invariant group were given the opportunity to develop talker-specific generative models for each VOT-stop mapping.
Recall that a generative model refers to a listener's mental representation of the distribution of a phonetic category (like /p/) over an acoustic cue (like VOT) under the ideal adapter framework.
Since generative models are specific to pairs of cues and categories under this theory, listeners in the Invariant conditions had to organize their representations of VOT for each onset by talker.
By contrast, listeners in the Variant conditions had the option to integrate across talkers to organize each VOT-stop mapping at the accent level, since all three talkers produced exemplars of all onsets.
The fact that Direct-Invariant exposure reduced lexical competition during test means that competition between voiced and voiceless stop contrasts was reduced.
This reduction in perceptual ambiguity was the result of talker-specific generative models for voiceless stops in particular.
Together, these findings suggest that exposure-test similarity is necessary but not sufficient for talker-independent perceptual adaptation; rather, the sources of covariation also need to be reduced during exposure in order for similarity to facilitate generalization.

Finally, we return to the lack of significant differences between the Test-only and Direct-Invariant groups.
The fact that participants without exposure were able to perform at a similar level to those with exposure weakens the argument we put forward in the previous paragraph.
If exposure facilitates generalization, but generalization does not facilitate future performance, then what is the benefit of exposure?
However, there is a wealth of evidence that previous exposure to an L2 accent improves perception of a novel L2-accented talker with the same L1 [@bent2021].
Previous studies have generally used either sentence transcription [e.g., @bradlow2008] or primed lexical decision [e.g., @xie2017similarity] to test the strength of adaptation.
Here, we used a matching task, under the assumption that categorization of short lag VOTs as voiceless stops should change as a function of perception of short lag VOTs as voiceless stops.
For example, consider the auditory prime *park* and the visual target *bark*.
Participants needed to decide whether the onset of the token they had heard was a /b/ or not.
If they accurately perceived the onset as a /p/, then they would correctly reject *bark* as a match.
Thus, accuracy on the matching task was the outcome of the categorization process.
It is possible that this outcome-based measure was not fine-grained enough to capture subtle changes in perception.
To return to the Competitor target example, the lack of difference in categorizing Spanish-accented *park* as *bark* may belie differences in perceiving Spanish-accented /p/.
To better assess changes in perception, we changed the test task for Experiment 3.

